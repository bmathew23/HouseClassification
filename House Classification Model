---
title: "Assign3_Mathew"
author: "Binil Mathew-R11583642"
date: "2024-11-02"
output: word_document
---

```{r}
ub <- read.csv('C:/Users/mathe/Downloads/UniversalBanking.csv')
head(ub)
```
#1 
```{r}

library(caret)
set.seed(123)
fac_pl <- as.factor(ub$Personal.Loan)


# Create Train Partitions from UB
ub_partition <- createDataPartition(fac_pl, p = 0.7, list = FALSE)
ub_train.df <- ub[ub_partition, ]
ub_test.df <- ub[-ub_partition, ]

head(ub_train.df)
head(ub_test.df)

#Train Labels
trainLabel <- ub[ub_partition, 7]
testLabel <- ub[-ub_partition, 7]

#Install rpart Libraries
library(rpart)
library(rpart.plot)

# Cart Model
cart_model <- rpart(Personal.Loan ~ ., data = ub_train.df, method = "class")
rpart.plot(cart_model)
cart_model
summary(cart_model)

#Pruning choose lowest x error value to test which is 0.2153
pruning <- prune(cart_model, cp = 0.021, "CP")
rpart.plot(pruning, cex=0.7)
```
# When looking at the split nodes in our plot, we see splits between income<107, education<2, family<3 and income<115. This ultimately shows the blueprint of our branches. When looking at the cart model, we see there were 3500 different n's/values/observations, along with each split, and how many relevant observations fall under each split. For example, in node number 2, 2723/3500 initial observations were used etc. It also helps us see which variables were deemed most important being income, and education (35 each). Additionaly, it shows us the numbers associated with those who took out personal loans vs those who didnt, along with our relative error and cross validation errors. I also calculated the accuracy of our cart model, and it came out to 98% which as you will see later is lower then our Random Forest model.


#2
```{r}
# Need to make variables a factor not int
ub_train.df$Personal.Loan <- as.factor(ub_train.df$Personal.Loan)
ub_test.df$Personal.Loan <- as.factor(ub_test.df$Personal.Loan)

# Bagging
library(ipred)
set.seed(123)
ub_bagging <- bagging(Personal.Loan~ ., data=ub_train.df, nbagg=25)
bagging_prediction <- predict(ub_bagging, ub_test.df)
bagging_table <- table(ub_test.df$Personal.Loan, bagging_prediction)
bagging_table

# Boosting
library(adabag)
set.seed(300)
ub_boosting <- boosting(Personal.Loan~ ., data=ub_train.df)
boost_prediction <- predict(ub_boosting, ub_test.df)
boost_table <- table(ub_test.df$Personal.Loan, boost_prediction$class)
boost_table

# Random Forests
library(randomForest)
train <- sample(1:150, 100)
rf_ub <- randomForest(Personal.Loan ~ .,nodesize =3, mtry =2, ntree = 15, data = ub_train.df)
rf_prediction <- predict(rf_ub, ub_test.df)
rf_table <- table(ub$Personal.Loan[-ub_partition], rf_prediction)
rf_table

# Cart Accuracy
cart_prediction <- predict(cart_model, ub_test.df, type = "class")
cart_table <- table(ub_test.df$Personal.Loan, cart_prediction)
cart_table

```
# Upon looking at the three different types of ensemble models, accuracy wise, it seems the rf prediction comes out with the highest accuracy percentage of 98.41%. This is due to the fact that random forest takes multiple decision trees to reach a result. It contains elements of bagging and boosting, and ultimately comes out more effective then both. Additionally, when calculating miscalcuations, RF had 27, while boosting had 31 and bagging had 28. Therefore, I think random forest is the best model here. Comparing these to the cart model, the cart model has a misclassification value of 30, which is higher then our random forest prediction model. 

#3
```{r}
caroline <- data.frame(Age = 32, Experience = 6, Education = 3, Income = 200000, Family = 3, Mortgage = 0, Securities.Account = 1, CreditCard = 1)

caroline_predicition <- predict(rf_ub, caroline)
caroline_predicition

```
# Based off our prediction model, Caroline would accept a personal loan. The prediction model returned a value of 1 meaning she would accept it! I decided to go with the random forest model because it had the highest level of accuracy, and least amount of misclassifications. 
